{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df1ac2b-da99-42c5-8317-3b5c2bdc2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Name: /physical_device:GPU:0 Type: GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ba3dc4-d876-4b5c-82ef-660f13b92a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique users: 51728\n",
      "No of unique movies: 9029\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ratings = pd.read_parquet('ratings_data.parquet')\n",
    "\n",
    "def mapping(x):\n",
    "    if x >= 4.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_ratings['implicit_feedback'] = df_ratings['rating'].apply(mapping)\n",
    "\n",
    "df_ratings = df_ratings[df_ratings['implicit_feedback'] == 1].reset_index(drop=True)\n",
    "\n",
    "# Filtering users who only have more than 20 positive interaction\n",
    "\n",
    "positive_interaction_per_user = df_ratings[df_ratings['implicit_feedback'] == 1] \\\n",
    "                                .groupby('user_id')['movie_id'].count()\n",
    "\n",
    "rare_users = positive_interaction_per_user[positive_interaction_per_user < 20] \\\n",
    "                                    .index.tolist()\n",
    "\n",
    "df_ratings = df_ratings[~df_ratings['user_id'].isin(rare_users)]\n",
    "\n",
    "# Filtering out rare movies\n",
    "\n",
    "movie_interaction_per_user = df_ratings.groupby('movie_id')['user_id'].count()\n",
    "\n",
    "rare_movies = movie_interaction_per_user[movie_interaction_per_user < 4] \\\n",
    "                                    .index.tolist()\n",
    "\n",
    "df_ratings = df_ratings[~df_ratings['movie_id'].isin(rare_movies)]\n",
    "\n",
    "# Encoding user and movie id's to a continous scale as expectd by NCF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "df_ratings['user_id_encoded'] = user_encoder.fit_transform(df_ratings['user_id'])\n",
    "df_ratings['movie_id_encoded'] = movie_encoder.fit_transform(df_ratings['movie_id'])\n",
    "\n",
    "\n",
    "# no of unique users in our data\n",
    "\n",
    "print('No of unique users:', df_ratings['user_id_encoded'].nunique())\n",
    "\n",
    "# no of unique movies in our data\n",
    "\n",
    "print('No of unique movies:', df_ratings['movie_id_encoded'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3dd9a-695b-4e4b-b08b-c7261b7e5971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5790ac-b8d8-4cd0-aa71-e6f10c8dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating val-dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "val_df = df_ratings.groupby('user_id_encoded', group_keys=False).sample(1, random_state=42)\n",
    "\n",
    "val_indices = val_df.index\n",
    "\n",
    "train_df = df_ratings.drop(val_indices).reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf68ed9-8c43-4291-96ce-44844c3cc8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7c7b1d-0bdf-4a05-9778-025edecea7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users Sampling: 100%|██████████| 51728/51728 [00:53<00:00, 959.66it/s] \n"
     ]
    }
   ],
   "source": [
    "# Add Negative Sampling for users for movies they have not interacted with in train data \n",
    "\n",
    "user_rated_movies = train_df.groupby('user_id_encoded')['movie_id_encoded'].apply(set).to_dict()\n",
    "all_movies = set(train_df['movie_id_encoded'].unique())\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "NEGATIVE_RATIO = 3\n",
    "neg_samples = []\n",
    "\n",
    "for user in tqdm(train_df['user_id_encoded'].unique(), desc=\"Users Sampling\"):\n",
    "    user_watched_movies = user_rated_movies.get(user, set())\n",
    "    n_positives = len(user_watched_movies)\n",
    "    n_negatives = n_positives * NEGATIVE_RATIO\n",
    "    candidates = np.array(list(all_movies - user_watched_movies))\n",
    "    if len(candidates) == 0 or n_negatives == 0:\n",
    "        continue\n",
    "    n_samples = min(n_negatives, len(candidates))\n",
    "    neg_movies = np.random.choice(candidates, size=n_samples, replace=False)\n",
    "    neg_samples.append(\n",
    "        pd.DataFrame({\n",
    "            'user_id_encoded': [user]*n_samples,\n",
    "            'movie_id_encoded': neg_movies,\n",
    "            'implicit_feedback': [0]*n_samples\n",
    "        })\n",
    "    )\n",
    "\n",
    "df_negatives = pd.concat(neg_samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2fad5-d051-4f37-99c1-68a5ef98ed1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd331c2-4ce6-4067-ae5d-888b04852aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with positive + negative samples\n",
    "\n",
    "train_data_final = pd.concat([\n",
    "    train_df[['user_id_encoded', 'movie_id_encoded' ,'implicit_feedback']],\n",
    "    df_negatives\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c91901-d7a4-45eb-af49-0c14cecffbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a11ac8e-d45b-4834-9d80-89998c0c23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating val users: 100%|██████████| 51728/51728 [01:23<00:00, 621.44it/s]\n"
     ]
    }
   ],
   "source": [
    "NEGATIVE_RATIO = 2\n",
    "\n",
    "# Create validation dataset with negatives\n",
    "val_neg_samples = []\n",
    "for user in tqdm(val_df['user_id_encoded'].unique(), desc='creating val users'):\n",
    "    user_watched_movies = user_rated_movies.get(user, set()) | set(val_df[val_df['user_id_encoded'] == user]['movie_id_encoded'])\n",
    "    candidates = list(all_movies - user_watched_movies)\n",
    "    if not candidates:\n",
    "        continue\n",
    "    n_negatives = NEGATIVE_RATIO  # 2 negatives per positive\n",
    "    n_samples = min(n_negatives, len(candidates))\n",
    "    neg_movies = np.random.choice(candidates, size=n_samples, replace=False)\n",
    "    val_neg_samples.append(\n",
    "        pd.DataFrame({\n",
    "            'user_id_encoded': [user] * n_samples,\n",
    "            'movie_id_encoded': neg_movies,\n",
    "            'implicit_feedback': [0] * n_samples\n",
    "        })\n",
    "    )\n",
    "\n",
    "df_val_negatives = pd.concat(val_neg_samples, ignore_index=True) if val_neg_samples else pd.DataFrame()\n",
    "val_df_with_neg = pd.concat([\n",
    "    val_df[['user_id_encoded', 'movie_id_encoded', 'implicit_feedback']],\n",
    "    df_val_negatives\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3a912-9d83-483a-b8e7-6d6423a0b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "# Preparing validation dataset for loss\n",
    "\n",
    "val_users = np.array(val_df_with_neg['user_id_encoded'], dtype=np.int32)\n",
    "val_movies = np.array(val_df_with_neg['movie_id_encoded'], dtype=np.int32)\n",
    "val_labels = np.array(val_df_with_neg['implicit_feedback'], dtype=np.float32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((val_users, val_movies), val_labels)\n",
    ")\n",
    "val_dataset = val_dataset.batch(batch_size=256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Verify validation data\n",
    "print(\"Validation samples with negatives:\", len(val_df_with_neg))\n",
    "print(\"Validation data balance:\\n\", val_df_with_neg['implicit_feedback'].value_counts())\n",
    "val_movies_missing = set(val_df['movie_id_encoded'].unique()) - all_movies\n",
    "print(\"Validation movies missing in training:\", len(val_movies_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da8186-e935-403e-b6d0-4be02442b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "# Prepare training dataset\n",
    "train_users = np.array(train_data_final['user_id_encoded'])\n",
    "train_movies = np.array(train_data_final['movie_id_encoded'])\n",
    "train_labels = np.array(train_data_final['implicit_feedback'])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((train_users, train_movies), train_labels)\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100000, seed=42)\n",
    "train_dataset = train_dataset.batch(batch_size=256).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21d09b-31bd-4433-8a52-1200dd1b92e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b773005-aadb-4506-8553-b242b3fcaff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 51728 Num items: 9029\n",
      "Original movies: 9029\n",
      "Training movies: 9029\n",
      "Final training movies: 9029\n",
      "All movies set: 9029\n",
      "Epoch 1/6\n",
      "73679/73689 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8624\n",
      "Epoch 1: val_loss improved from inf to 0.33200, saving model to new_best_model.h5\n",
      "73689/73689 [==============================] - 182s 2ms/step - loss: 0.3364 - accuracy: 0.8624 - val_loss: 0.3320 - val_accuracy: 0.8624\n",
      "Epoch 2/6\n",
      "73672/73689 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8697\n",
      "Epoch 2: val_loss improved from 0.33200 to 0.32285, saving model to new_best_model.h5\n",
      "73689/73689 [==============================] - 181s 2ms/step - loss: 0.3226 - accuracy: 0.8697 - val_loss: 0.3229 - val_accuracy: 0.8758\n",
      "Epoch 3/6\n",
      "73679/73689 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8811\n",
      "Epoch 3: val_loss improved from 0.32285 to 0.30861, saving model to new_best_model.h5\n",
      "73689/73689 [==============================] - 180s 2ms/step - loss: 0.3110 - accuracy: 0.8811 - val_loss: 0.3086 - val_accuracy: 0.8880\n",
      "Epoch 4/6\n",
      "73678/73689 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8874\n",
      "Epoch 4: val_loss improved from 0.30861 to 0.29900, saving model to new_best_model.h5\n",
      "73689/73689 [==============================] - 181s 2ms/step - loss: 0.3035 - accuracy: 0.8874 - val_loss: 0.2990 - val_accuracy: 0.8942\n",
      "Epoch 5/6\n",
      "73675/73689 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8901\n",
      "Epoch 5: val_loss improved from 0.29900 to 0.29618, saving model to new_best_model.h5\n",
      "73689/73689 [==============================] - 182s 2ms/step - loss: 0.3014 - accuracy: 0.8901 - val_loss: 0.2962 - val_accuracy: 0.8975\n",
      "Epoch 6/6\n",
      "73680/73689 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8912\n",
      "Epoch 6: val_loss did not improve from 0.29618\n",
      "73689/73689 [==============================] - 182s 2ms/step - loss: 0.3019 - accuracy: 0.8912 - val_loss: 0.2995 - val_accuracy: 0.8961\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " user_id_encoded (InputLayer)   [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " movie_id_encoded (InputLayer)  [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_embeddings (Embedding)    (None, 32)           1655296     ['user_id_encoded[0][0]']        \n",
      "                                                                                                  \n",
      " movie_embeddings (Embedding)   (None, 32)           288928      ['movie_id_encoded[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['user_embeddings[0][0]',        \n",
      "                                                                  'movie_embeddings[0][0]']       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           528         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            17          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,951,009\n",
      "Trainable params: 1,951,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "User embedding shape: (51728, 32)\n",
      "Movie embedding shape: (9029, 32)\n",
      "Final layer weights: [[ 5.088889 ]\n",
      " [ 2.269932 ]\n",
      " [ 1.5217506]\n",
      " [ 1.4469162]\n",
      " [-3.3276937]\n",
      " [ 2.5163195]\n",
      " [-3.7671473]\n",
      " [ 1.644555 ]\n",
      " [-2.7404058]\n",
      " [ 1.6370393]\n",
      " [-2.0034413]\n",
      " [ 4.263738 ]\n",
      " [ 4.6268277]\n",
      " [-1.3418453]\n",
      " [ 1.9489563]\n",
      " [ 1.8043869]]\n",
      "Final layer bias: [-0.30641854]\n",
      "73689/73689 [==============================] - 62s 840us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "\n",
    "num_users = train_data_final['user_id_encoded'].nunique()  # ~51728\n",
    "num_items = train_data_final['movie_id_encoded'].nunique()  # ~9029\n",
    "\n",
    "\n",
    "embedding_size = 32\n",
    "l2_reg = 1e-4\n",
    "\n",
    "user_input = layers.Input(shape=(), name='user_id_encoded', dtype=tf.int32)\n",
    "movie_input = layers.Input(shape=(), name='movie_id_encoded', dtype=tf.int32)\n",
    "\n",
    "user_embedding = layers.Embedding(num_users, embedding_size, name='user_embeddings',\n",
    "                                 embeddings_regularizer=regularizers.l2(l2_reg))(user_input)\n",
    "movie_embedding = layers.Embedding(num_items, embedding_size, name='movie_embeddings',\n",
    "                                  embeddings_regularizer=regularizers.l2(l2_reg))(movie_input)\n",
    "\n",
    "interaction = layers.Concatenate()([user_embedding, movie_embedding])\n",
    "\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(interaction)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "train_users = np.array(train_data_final['user_id_encoded'], dtype=np.int32)\n",
    "train_movies = np.array(train_data_final['movie_id_encoded'], dtype=np.int32)\n",
    "train_labels = np.array(train_data_final['implicit_feedback'], dtype=np.float32)\n",
    "\n",
    "val_users = np.array(val_df_with_neg['user_id_encoded'], dtype=np.int32)\n",
    "val_movies = np.array(val_df_with_neg['movie_id_encoded'], dtype=np.int32)\n",
    "val_labels = np.array(val_df_with_neg['implicit_feedback'], dtype=np.float32)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint('new_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    [train_users, train_movies], train_labels,\n",
    "    validation_data=([val_users, val_movies], val_labels),\n",
    "    epochs=6, batch_size=256, verbose=1,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Predict scores\n",
    "all_scores = model.predict([train_users, train_movies], batch_size=256, verbose=1).flatten()\n",
    "\n",
    "# Add to DataFrame\n",
    "train_data_final['ncf_score_2'] = all_scores\n",
    "\n",
    "# Check score distribution\n",
    "print(\"Score statistics:\")\n",
    "print(\"Max:\", train_data_final['ncf_score_2'].max())\n",
    "print(\"Mean:\", train_data_final['ncf_score_2'].mean())\n",
    "print(\"Positives mean:\", train_data_final[train_data_final['implicit_feedback'] == 1]['ncf_score_2'].mean())\n",
    "print(\"Negatives mean:\", train_data_final[train_data_final['implicit_feedback'] == 0]['ncf_score_2'].mean())\n",
    "\n",
    "# Subset test\n",
    "subset = train_data_final.sample(1000, random_state=42)\n",
    "user_ids_subset = np.array(subset['user_id_encoded'], dtype=np.int32)\n",
    "movie_ids_subset = np.array(subset['movie_id_encoded'], dtype=np.int32)\n",
    "scores_subset = model.predict([user_ids_subset, movie_ids_subset], batch_size=256, verbose=1).flatten()\n",
    "print(\"Subset max score:\", scores_subset.max())\n",
    "print(\"Subset positives mean:\", scores_subset[subset['implicit_feedback'] == 1].mean())\n",
    "print(\"Subset negatives mean:\", scores_subset[subset['implicit_feedback'] == 0].mean())\n",
    "\n",
    "# Debug logits (pre-sigmoid)\n",
    "logit_model = Model(inputs=model.inputs, outputs=model.get_layer('output').get_output_at(0))\n",
    "logit_model.layers[-1].activation = None  # Remove sigmoid\n",
    "logits_subset = logit_model.predict([user_ids_subset, movie_ids_subset], batch_size=256, verbose=1).flatten()\n",
    "print(\"Logits min:\", logits_subset.min(), \"max:\", logits_subset.max())\n",
    "\n",
    "# Manually apply sigmoid\n",
    "manual_scores = 1 / (1 + np.exp(-logits_subset))\n",
    "print(\"Manual sigmoid max:\", manual_scores.max())\n",
    "print(\"Manual sigmoid positives mean:\", manual_scores[subset['implicit_feedback'] == 1].mean())\n",
    "print(\"Manual sigmoid negatives mean:\", manual_scores[subset['implicit_feedback'] == 0].mean())\n",
    "\n",
    "# # Verify evaluation\n",
    "# hr, ndcg = evaluate_model(model, val_df, user_rated_movies, all_movies, max_users=1000, num_negatives=100)\n",
    "# print(f\"Test HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5db267-1063-4831-8f40-509c7e8e7373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02072623-7da0-4e2d-a61b-cb96540f4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('new_best_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be652f-069c-4719-899b-4d07776f5371",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552fbbf1-f497-4616-96bf-17312f481041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation movies missing in training: 0\n",
      "Users with no non-rated items: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Verify data consistency\n",
    "val_movies = set(val_df['movie_id_encoded'].unique())\n",
    "all_movies = set(train_data_final['movie_id_encoded'].unique())\n",
    "missing_movies = val_movies - all_movies\n",
    "\n",
    "user_rated_movies = train_df.groupby('user_id_encoded')['movie_id_encoded'].apply(set).to_dict()\n",
    "skipped_users = sum(1 for user in val_df['user_id_encoded'].unique() if not (all_movies - user_rated_movies.get(user, set())))\n",
    "\n",
    "# Optimized evaluation function\n",
    "def evaluate_model(model, val_df, user_rated_movies, all_movies, num_negatives=50, k=10, max_users=None):\n",
    "    # Filter valid users\n",
    "    val_df = val_df[val_df['movie_id_encoded'].isin(all_movies)]\n",
    "\n",
    "    if val_df.empty:\n",
    "        print(\"No valid validation users after filtering.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    hr, ndcg = [], []\n",
    "    users_to_eval = val_df['user_id_encoded'].unique()\n",
    "\n",
    "    if max_users:\n",
    "        users_to_eval = users_to_eval[:max_users]\n",
    "    batch_size = 1000\n",
    "    skipped_users = 0\n",
    "\n",
    "    for start in range(0, len(users_to_eval), batch_size):\n",
    "        batch_users = users_to_eval[start:start + batch_size]\n",
    "        batch_users_list, batch_items_list = [], []\n",
    "        batch_pos_items, batch_neg_items = [], []\n",
    "\n",
    "        for user in batch_users:\n",
    "            pos_item = val_df[val_df['user_id_encoded'] == user]['movie_id_encoded'].values[0]\n",
    "            non_rated = list(all_movies - user_rated_movies.get(user, set()))\n",
    "            if not non_rated:\n",
    "                skipped_users += 1\n",
    "                continue\n",
    "            neg_items = np.random.choice(non_rated, size=min(num_negatives, len(non_rated)), replace=False)\n",
    "            items = np.array([pos_item] + list(neg_items))\n",
    "            users = np.array([user] * len(items))\n",
    "            \n",
    "            batch_users_list.extend(users)\n",
    "            batch_items_list.extend(items)\n",
    "            batch_pos_items.append(pos_item)\n",
    "            batch_neg_items.append(neg_items)\n",
    "\n",
    "        if not batch_users_list:\n",
    "            continue\n",
    "\n",
    "        scores = model.predict(\n",
    "            [np.array(batch_users_list, dtype=np.int32), np.array(batch_items_list, dtype=np.int32)],\n",
    "            verbose=0,\n",
    "            batch_size=256\n",
    "        ).flatten()\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        for i, pos_item in enumerate(batch_pos_items):\n",
    "            num_items = num_negatives + 1\n",
    "            user_scores = scores[idx:idx + num_items]\n",
    "            top_k_indices = np.argsort(user_scores)[::-1][:k]\n",
    "            items = np.array([pos_item] + list(batch_neg_items[i]))\n",
    "\n",
    "            top_k_items = items[top_k_indices]\n",
    "            hr.append(int(pos_item in top_k_items))\n",
    "\n",
    "            if pos_item in top_k_items:\n",
    "                rank = np.where(top_k_items == pos_item)[0][0]\n",
    "                ndcg.append(1.0 / np.log2(rank + 2))\n",
    "\n",
    "            else:\n",
    "                ndcg.append(0.0)\n",
    "\n",
    "            idx += num_items\n",
    "\n",
    "    if skipped_users:\n",
    "        print(f\"Skipped {skipped_users} users with no non-rated items.\")\n",
    "\n",
    "    return np.mean(hr) if hr else 0.0, np.mean(ndcg) if ndcg else 0.0\n",
    "\n",
    "# Prepare evaluation data\n",
    "user_rated_movies = train_df.groupby('user_id_encoded')['movie_id_encoded'].apply(set).to_dict()\n",
    "all_movies = set(train_data_final['movie_id_encoded'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c66f13-289e-44d6-b0cb-b037816f5d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test HR@10: 0.8674, NDCG@10: 0.5948\n"
     ]
    }
   ],
   "source": [
    "# Verify evaluation for all users\n",
    "hr, ndcg = evaluate_model(model, val_df, user_rated_movies, all_movies, max_users=None, num_negatives=100)\n",
    "print(f\"Test HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addd52ee-9875-43af-bb58-1fbb41188589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test HR@10: 0.8790, NDCG@10: 0.5930\n"
     ]
    }
   ],
   "source": [
    "# Verify evaluation for random 1000 users\n",
    "hr, ndcg = evaluate_model(model, val_df, user_rated_movies, all_movies, max_users=1000, num_negatives=100)\n",
    "print(f\"Test HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cde104-880f-4cd0-afcc-076b64328fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
